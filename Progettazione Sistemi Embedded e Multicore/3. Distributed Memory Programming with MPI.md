## #Chapter Distributed Memory Programming

###### A distributed memory system:

**![f03-01-9780123742605.eps](https://lh3.googleusercontent.com/wriWtYAPMusmUB7AuoOTz5WlMoihUD4Hmx5I_xW428GsuGYA4Zdkli6nsmuS0TC0t1w0a5nBIbZs6mpJXqTC2daokPDeQYIEcuP_CyzjdyzPPJv0J3B3yWRVcEVCUjyyC7htOEqBwDugSCb2C83m=s2048)**

### Identifying MPI (Message Passing Interface) processes
A common practice to identify processes is using nonnegative integer ranks: `p` processes are numbered `0, 1, 2, .. p-1`

### Our first MPI program
```c
#include <stdio.h>
#include <string.h> //for strlen
#include <mpi.h> //for MPI functions

const int MAX_STRING = 100;

int main(void){
	char greeting[MAX_STRING];
	int comm_sz; //Number of processes
	int my_rank; //My process rank
	
	MPI_Init(NULL, NULL);
	MPI_Comm_size(MPI_COMM_WORLD, &comm_sz);
	MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);
	
	if(my_rank != 0){
		sprintf(greeting, "Greetings from process %d of %d!", my_rank, comm_sz);
		MPI_Send(greeting, strlen(greeting)+1, MPI_CHAR, 0, 0, MPI_COMM_WORLD);
	} else{
		printf("Greetings from process %d of %d!\n", my_rank, comm_sz);
		for(int q = 1; q < comm_sz; q++){
			MPI_Recv(greeting, MAX_STRING, MPI_CHAR, q, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
			printf("%s\n", greeting);
		}
	}
	
	MPI_Finalize();
	return 0;
} //main
```
##### Compilation
```bash
mpicc -g -Wall -o mpi_hello mpi_hello.c
```
- `mpicc`: wrapper script to compile;
- `-g`: produce debugging information;
- `-Wall`: turns on all warnings;
- `-o mpi_hello`: create this executable file name (as opposed to default a.out);
- `mpi_hello.c`: source file;

##### Execution
```bash
mpiexec -n <number of processes> <executable>
```
###### ex.
```Shell
mpiexec -n 1 ./mpi_hello
```
*this runs with 1 process and will print:*
```shell
Greetings from process 0 of 1!
```

```shell
mpiexec -n 3 ./mpi_hello
```
*and this one with 3 and will print:*
```shell
Greetings from process 0 of 3!
Greetings from process 1 of 3!
Greetings from process 2 of 3!
```

### MPI Components
- MPI_Init: tells MPI to do all the necessary setup
```c
int MPI_Init(
	int*    argc_p // in/out
	char*** argv_p // in/out
)
```

- MPI_Finalize: tells MPI we're done, so clean up anything allocated for this program
```c
int MPI_Finalize(void);
```

### Basic Outline
```c
...
#include <mpi.h>
...
int main(int argc, char* argv[]){
	...
	//No MPI calls before this
	MPI_Init(&argc, &argv);
	...
	MPI_Finalize();
	//No MPI calls after this
	...
	return 0;
}
```

### Communicators
These things are a collection of processes that can send messages to each other. `MPI_Init` defines a communicator that consists of all the processes created when the program is started called `MPI_COMM_WORLD`.

```c
int MPI_Comm_size(
	MPI_Comm comm       //in
	int*     comm_sz_p  //out
)
```
*`comm_sz_p` represents the number of processes in the communicator*

```c
int MPI_Comm_rank(
	MPI_Comm comm       //in
	int*     my_rank_p  //out
)
```
*`my_rank_p` represents my rank (the process making this call)*

Suppose you have 2 MPI independent libraries of functions, they don't communicate with each other, but they do communicate internally. We can do it with tags, assigning tags `
`[1, n]` and tags `[n+1, m]` tot the two libraries, or we can simply pass one communicator to one library functions and a different communicator to the other library.
### SPMD
As said it stands for Single-Program Multiple-Data, in which we compile one program.
Process 0 receives messages and prints them while the others do the work.
The *if-else* construct makes our program SPMD.

### Communication
```c
int MPI_Send(
	void*         msg_buf_p    //in
	int           msg_size     //in
	MPI_Datatype  msg_type     //in
	int           dest         //in
	int           tag          //in
	MPI_Comm      communicator //in
)
```

```c
int MPI_Recv(
	void*         msg_buf_p    //out
	int           msg_size     //in
	MPI_Datatype  msg_type     //in
	int           source       //in
	int           tag          //in
	MPI_Comm      communicator //in
	MPI_Status*   status_p     //out
)
```

### Message Matching
```c
MPI_Send(send_buf_p, send_buf_sz, send_type, dest, send_tag, send_comm);

MPI_Recv(recv_buf_p, recv_buf sz, recv_type, src, recv_tag, recv_comm, &status);
```
in which:
- `send_comm` = `recv_comm`;
- `send_tag` = `recv_tag`;
- `dest` = recv id;
- `src` = send id;

The message is *successfully* received if:
- `recv_type` = `send_type`
- `recv_buf_sz` > `send_buf_sz`

A receiver can get a message without knowing:
- the amount of data in the message;
- the sender of the message (MPI_ANY_SOURCE);
- the tag of the message (MPI_ANY_TAG);

`status` is initialized:
```c
MPI_Status* status;
```
and can be:
```c
status.MPI_SOURCE
status.MPI_TAG
status.MPI_ERROR
```

### How much data am I receiving?
```c
int MPI_Get_count(
	MPI_Status*    status_p   //in
	MPI_Datatype   type       //in
	int*           count_p    //in
)
```

## #Chapter Trapezoidal Rule in MPI

### The Trapezoidal Rule
**![](https://lh3.googleusercontent.com/sPZcocVjS622P9pP0trgtYCsgo3NQ-ssbH1nuIUluRrB3A1pzUJeKEt9xHXc49l8W8eWULgl4qiYTxIRwvGu8pp6pye3M-XQ8NNYHAPSZrlymwVLyDVg8pLwULcQf1FEU11yb_HTn2UYjuGKqvQ2=s2048)**

Area of one trapezoid: $$A=\frac{h}{2}[f(x_i)+f(x_{i+1})]$$
$$h=\frac{b-a}{n}$$
$$x_0=a,x_1=a+h,x_2=a+2h,...,x_{n-1} =a+(n-1)h,x_n=b$$
Sum of trapezoid areas: $$h[\frac{f(x_0)}{2}+f(x_1)+f(x_2)+...+f(x_{n-1})+\frac{x_n}{2}]$$
One trapezoid: 
** ![f03-04-9780123742605.eps](https://lh4.googleusercontent.com/QGzBfjbOmLM3bPKjdyYAVpJWaOUbluJat85VQIIg0jeFGJzb_oY1kyMtCksjwJKIJjqpBaLu1L9eeVnxrVMJlBxHpDNhFLeMZIKgJPsZugGbutMiaPAtoa4FX23DyvissCWKqmQjO72GWdwtCn5u=s2048) **

###### Pseudo-code for a serial program
```pseudo-code
//input: a, b, n
h = (b-a)/n;
approx = (f(a)+f(b))/2.0;
for(i=1; i<=n-1; i++){
	x_i = a + i*h;
	approx += f(x_i);
}
approx = h*approx;
```

### Parallelizing the Trapezoidal Rule
1. Partition problem solution into tasks;
2. Identify communication channels between tasks;
3. Aggregate tasks into composite tasks;
4. Map composite tasks to cores.

###### Parallel pseudo-code
```pseudo-code
Get a, b, n;
h = (b-a)/n;
local_n = n/comm_sz;

local_a = a + my_rank*local_n*h;
local_b = local_a + local_n*h;
local_integral = Trap(local_a, local_b, local_n, h);
if(my_rank!=0){
	Send local_integral to process 0;
} else{
	total_integral = local_integral;
	for(proc = 1; proc < comm_sz; proc++){
		Rceive local_integral from proc;
		total_integral += local_integral;
	}
}
if(my_rank == 0){
	print result;
}
```

###### Ex.
```c
int main(void){
	int my_rank, comm_sz, n = 1024, local_n;
	double a = 0.0, b= 3.0, h, local_a, local_b;
	double local_int, total_int;
	int source;
	
	MPI_Init(NULL, NULL);
	MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);
	MPI_Comm_size(MPI_COMM_WORLD, &comm_sz);
	
	h = (b-a)/n;         //h is the same for all processes
	local_n = n/comm_sz;  //so is the number of trapezoids
	
	local_a = a + my_rank*local_n*h;
	local_b = local_a+local_n*h;
	local_int = Trap(local_a, local_b, local_n, h);
	
	if(my_rank != 0){
		MPI_Send(&local_int, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);
	} else{
		total_int = local_int;
		for(source = 1; source<comm_sz; source++){
			MPI_Recv(&local_int, 1, MPI_DOUBLE, source, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
			total_int += local_int;
		}
	}
	
	if(my_rank == 0){
		printf("With n = %d trapezoids, our estimate\n", n);
		printf("of the integral from %f to %f = %.15e\n", a, b, total_int);
	}
	MPI_Finalize();
	return 0;
} //main

double Trap(
		double left_endpt,  //in
		double right_endpt, //in
		int    trap_count,  //in
		double base_len){   //in
	double estimate, x;
	int i;
	
	estimate = (f(left_endpt)+f(right_endpt))/2.0;
	for(i 0 1; i <= trap_count-1; i++){
		x = left_endpt + i*base_len;
		estimate += f(x);
	}
	estimate = estimate*base_len;
return estimate
} //Trap
```

### Dealing with I/O
...


### Alternative Point-to-Point Communication Modes
MPI_Send uses the standard communication mode. What it does depends on the size of the message, if the size is small enough, it becomes locally blocking, otherwise globally.
There are three additional communication modes:
- Buffered: in this mode the sending operation is always locally blocking.
- Synchronous: When the message is big enough the sending operation will return only after the destination process has started the retrieval of the message (*globally blocking*);
- Ready: the send operation will succeed only if a matching receive operation has been initiated already.

### Buffered Communications
###### Typical setup 
```c
MPI_Buffer_attach(...);
...
MPI_Bsend(...);
...
MPI_Budder_detach(...);
```
`MPI_Buffer_detach` returns the address and size of the buffer that was used by MPI.

### Buffered "Hello World" Example
```c
int rank, num, i;
MPI_Init(&argc, &argv);
MPI_Comm_rank (MPI_COMM_WORLD, &rank);
MPI_Comm_size (MPI_COMM_WORLD, &num);

if (rank == 0){
	//allocate buffer space and designate it for MPI use
	unsigned char* buff = (unsigned char*)malloc(sizeof(unsigned char)*COMMBUFFSIZE);
	MPI_Buffer_attach(buff, COMMBUFFSIZE);
	char* msg = "Test msg";
	for(i=1;i<num;i++){
		MPI_Bsend(msg, strlen(msg)+1, MPI_CHAR, i, MSGTAG, MPI_COMM_WORLD);
	}
	//detach and release buffer space
	unsigned char* bptr;
	int bsize;
	MPI_Buffer_detach(&bptr, &bsize);
	free(bptr);
} else{
	MPI_Status status;
	char msg[MAXMSGSIZE];
	MPI_Recv(msg, MAXMSGSIZE, MPI_CHAR, 0, MSGTAG, MPI_COMM_WORLD,...
```

### Non-blocking Communications
Buffered sends are considered *bad* for performance, instead the *non-blocking* (or immediate), maximize concurrency by returning immediately upon initiating a transfer, allowing communication and computation to overlap.
There are both Send and Receive immediate variants.

The downside is that the completion of operations for both end-points, has to be queried explicitely:
- for senders so that they can re-use or modify the message buffer.
- for receivers so that they can extract the message contents

### Immediate Send Function
```c
typedef int MPI_Request;
int MPI_Send(void*        buf,       //out
			 int          count,     //in
			 MPI_Datatype datatype,  //in
			 int          source,    //in
			 int          tag,       //in
			 MPI_Comm     comm,      //in
			 MPI_Request* req        //out
)
```
The `MPI_Request` that is returned, is a handle that allows a query on the status of the operation to take place.

### Status Polling
Blocking (destroys handle):
```c
int MPI_Wait(MPI_Request*  req,    //in/out
			 MPI_Status*   st     //out
			)
```

Non-blocking:
```c
int MPI_Test(MPI_Request*  req,    //in
			 int*          flag,   //out
			 MPI_Status*   st     //out
			)
```

### MPI_Reduce
```c
int MPI_Reduce(void*         input_data_p,    //in
			   void*         output_data_p,   //out
			   int           count,           //in
			   MPI_Datatype  datatype,        //in
			   MPI_Op        operator,        //in
			   int           dest_process,    //in
			   MPI_Comm      comm
                           );
```
`MPI_Reduce(&local_int, &total_int, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
`
### Predefined reduction operators in MPI
|Operation Value|Meaning|
|:-------------|:---------|
|MPI_MAX|Maximum|
|MPI_MIN|Minimum|
|MPI_SUM|Sum|
|MPI_PROD|Product|
|MPI_LAND|Logical and|
|MPI_BAND|Bitwise and|
|MPI_LOR|Logical or|
|MPI_BOR|Bitwise or|
|MPI_LXOR|Logical exclusive or|
|MPI_BXOR|Bitwise exclusive or|
|MPI_MAXLOC|Maximum and location of maximum|
|MPI_MINLOC|Minimum and location of minimum|

### Collective vs. Point-to-Point Communicators
*All* the processes in the communicator must call the same collective function.
For example, a program that attempts to match a call to `MPI_Reduce` on one process with a call to `MPI_Recv` on another process is erroneous, and, in all likelihood, the program will hang or crash.

The arguments passed by each process to an MPI collective communication must be “compatible.”
For example, if one process passes in 0 as the `dest_process` and another passes in 1, then the outcome of a call to `MPI_Reduce` is erroneous, and, once again, the program is likely to hang or crash.

The `output_data_p` argument is only used on `dest_process`.
However, all of the processes still need to pass in an actual argument corresponding to `output_data_p`, even if it’s just `NULL`.
Point-to-point communications are matched on the basis of tags and communicators.
*Collective* communications *don’t* use tags, they’re matched solely on the basis of the communicator and the order in which they’re called.

### MPI_Allreduce
Useful in a situation in which all of the processes need the result of a global sum in order to complete some larger computation.
```c
int MPI_Allreduce(void*        input_data_p,    //in
				  void*        output_data_p,   //out
				  int          count,           //in
				  MPI_Datatype datatype,        //in
				  MPI_Op       operator,        //in
				  MPI_Comm     comm             //in  
)
```

### Broadcast
Data belonging to a single process is sent to all of the processes in the communicator.
```c
int MPI_Bcast(void*        data_p,      //in/out
			  int          count,       //in
			  MPI_Datatype datatype,    //in
			  int          source_proc, //in
			  MPI_Comm     comm         //in
)
```

###### A version of Get_input that uses MPI_Bcast
```c
void Get_input(int     my_rank,   //in
			   int     comm_sz,   //in
			   double* a_p,       //out
			   double* b_p,       //out
			   int*    n_p,       //out
			   ){
	if(my_rank == 0){
		printf("Enter a, b, and n\n");
		scanf("%lf %lf %d", a_p, b_p, n_p);
	}
	MPI_Bcast(a_p, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);
	MPI_Bcast(b_p, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);
	MPI_Bcast(n_p, 1, MPI_INT, 0, MPI_COMM_WORLD);
}
```