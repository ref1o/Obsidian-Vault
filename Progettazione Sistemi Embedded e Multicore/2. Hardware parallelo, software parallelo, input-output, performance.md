## #Chapter Parallel Hardware

### Vector processors
They can operate on arrays or *vectors* of data while conventional CPUs operate on individual data elements or scalars. *Vector registers* are capable of storing a vector of operands and operating simultaneously on their contents and their functional units are vectorized and pipelined.
*Vector instructions* can operate on vectors rather than scalars and in addition, they have *interleaved memory*; multiple "banks" of memory, which can be accessed more or less independently: we can distribute elements of a vector across multiple banks, reducing or eliminating delay in loading/storing successive elements.
##### Pros:
- Fast
- Easy to use
- Vectorizing compilers are good at identifying code to exploit
- Compilers also can provide information about code that cannot be vectorized
- High memory bandwidth
- Uses every item in a cache line
##### Cons:
- They don’t handle irregular data structures as well as other parallel architectures
- A very finite limit to their ability to handle ever-larger problems. (scalability)

### Aurora NEC
Renews the vector processor concept using it as an *accelerator*, it uses standard form factor and interconnections like *PCIe* and it's suitable for High Performance Computing Applications

### GPUs: Throughput-Oriented Design 
They have *small caches* to boost memory throughput; they have simple controls like no branch predictions and no data forwarding; they have energy-efficient ALUs and they require a massive number of threads to tolerate latencies.

### GPUs: Latency-Oriented Design 
They have *large caches* and sophisticated controls like branch predictions for reduced branch latency and data forwarding for reduced data latency and finally powerful ALU for reduced operation latency

## #Chapter Parallel Software
### Writing Parallel Programs
Consists of dividing the work among the processes/threads so each process/thread gets roughly the same amount of work and communication is minimized; arrange for the processes/threads to *synchronize* and arrange for communication among processes/threads.

### SPMD - Single Program Multiple Data
A SPMD program consists of a single executable that can behave as if it were multiple different programs through the use of conditional branches
```c
if(im thread process i){
	do this;
} else{
	do that;
}
```

###### Ex. busy-waiting
```c
my_val = Compute_val (my_rank) ;
if(my_rank == 1){
     while(!ok_for_1) ;  /* Busy−wait loop */
}
x += my_val ;  /* Critical section */
if(my_rank == 0){
     ok_for_1 = true ;  /* Let thread 1 update x */
}
```

###### Ex. message-passing
```c
char message [100];
. . .
my_rank = Get_rank ();
if(my_rank == 1){
     sprintf(message, "Greetings from process 1");
     Send(message, MSG_CHAR, 100, 0);
} else if(my_rank == 0){
     Receive(message, MSG_CHAR, 100, 1);
     printf("Process 0 > Received: %s\n", message);
}
```

## #Chapter Input and Output

In *distributed-memory* programs, only process 0 will access `stdin`. In *shared-memory* programs, only the master thread or thread 0 will access `stdin`.
In both distributed and shared memory programs, all the processes/threads can access `stdout` and `stderr`.

However, because of the indeterminacy of the order of output to `stdout`, in most cases, only a single process/thread will be used for all output to `stdout` other than debugging output.
Debug output should always include the *rank* or *id* of the process/thread that's generating the output.

Only a single process/thread will attempt to access any single file other than `stdin`, `stdout`, or `stderr`. So, for example, each process/thread can open its own, private file for reading or writing, but no two processes/threads will open the same file.

## #Chapter Performance

### Speedup
- Number of cores = $p$
- Serial run-time = $T_{serial}$
- Parallel run-time = $T_{parallel}$
 $$T_{parallel}= \frac{T_{serial}}{p}$$

###### Speedup of a parallel program:
$$S = \frac{T_{serial}}{T_{parallel}}$$
###### Efficiency of a parallel program
$$E = \frac{S}{p}=\frac{(\frac{T_{serial}}{T_{parallel}})}{p}=\frac{T_{serial}}{p *T_{parallel}}$$

###### Speedups and efficiencies of a parallel program

**![](https://lh4.googleusercontent.com/YjIfSXY_EYIwkdGuG28OQVakmIideECVcF_KOzidXfB_99pNABn_Tf11m7Io2xRcdW4DlOkuTla-A9niGjSI_He7dhB6FOsWE39gC6ccSbIc6YIRvj27QoRrPo-7vw9oXsYRclEJFU_QP2NvCnFBSQ=s2048)

###### Speedups and efficiencies of parallel program on different problem sizes

**![](https://lh6.googleusercontent.com/UlIcn_9wqUkA3rLorIgY5q1cuzRm-IZ-vFidO14kq14xgcdLrKpzmAsfhE0G2QuLWXNyrrMZY82rOEZr3U3ynCuyJj67VSolLtT6kEZYAu-w6UQWISJoEhkJmOi8cc6aaLIz8BEg8m6rhM97hyBMRw=s2048)**

### Scalability
In general, a problem is *scalable* if it can handle ever-increasing problem sizes.
- If we increase the number of processes/threads and keep the <b>efficiency</b> fixed <u>without increasing the problem size</u>, the problem is <i>strongly scalable</i>.
- If we keep the <b>efficiency</b> fixed <u>increasing the problem size</u> at the same rate as we increase the number of processes/threads, the problem is <i>weakly scalable</i>.

### Strong and Weak Scaling

[[Amdhal's Law]]: Strong Scaling
- Fixed Problem Size
- How much does parallelism reduce the execution time of a problem?

[[Gustafson's Law]]: Weak Scaling
- Fixed Execution Time
- How much longer does it take for the problem without parallelism?

###### Amdahl's law and strong scaling

**![](https://lh3.googleusercontent.com/vd61s7LgydUCIwjMJCq-HJ2HDSpGMSqVvhClVio4BYn2mAcpfNHr6wFBdpRA_Uzb7D1swfelbSxbD3IRarXKH8wnIaNB5HKQLj6gXjMHLXW2sdOnota0VSkqPj_XURyqish8ZKxi-YPvCxIk4gOfTA=s2048)**
$$S = 1-\alpha$$
###### Gustafson's law and weak scaling

**![](https://lh5.googleusercontent.com/2TokwVb1PdTv31kb1I3-h9GYLYgL0wYr47yTkRpgVDXhIo6iH6NWS2_jjAg3KC-IRwlwIAuSBUqeIEx_xn_3U9lc4jMK2DvZSzq_jymfoZsg_EIJ3Lg8plb9MuVZ7-IAUfupf2ZpdTLKitsY5rFRiQ=s2048)**
$$S=1-\alpha$$
### Taking Times

###### Ex.
```c
double start, finish;
...
start= Get_current_time();
//code that we want to time
...
finish= Get_current_time();
printf("The elapsed time = %e seconds\n", finish-start);
```
*this is a general example of timing a piece of code*

Now we can implement processes/threads:
```c
double global_elapsed;
double my_start, my_finish, my_elapsed;
...
//Synchronize all processes/threads
Barrier();
my_start = Get_current_time();
//code that we want to time
...
my_finish = Get_current_time();
my_elapsed = my_finish-my_start;
//find the max across al processes/threads
global_elapsed = Global_max(my_elapsed);
if(my_rank==0){
	printf("The elapsed time = %e seconds\n", global_elapsed);
}
```
*variables that starts with `global_` are the ones shared across processes/threads, instead the `my_` variables are private in their own process/thread*

### Foster's methodology
1. *Partitioning*: divide the computation to be performed and the data operated on by the computation into smaller tasks. Here the focus should be on identifying tasks that can be executed in parallel.
2. *Communication*: determine what communication needs to be carried out among the tasks identified in the previous step.
3. *Agglomeration or aggregation*: combine tasks and communications identified in the first step into larger tasks. For example, if task A must be executed before task B can be executed, it may make sense to aggregate them into a single composite task.
4. *Mapping*: assign the composite tasks identified in the previous step to processes/threads. This should be done so that communication is minimized, and each process/thread gets roughly the same amount of work.

### Program Structure Patterns
We can distinguish the parallel program structure patterns into two major categories:
- <b>Globally Parallel, Locally Sequential (GPLS)</b>: this means that the application is able to perform multiple tasks concurrently, with each task running sequentially. Patterns that fall in this category include:
	- Single-Program, Multiple Data
	- Multiple-Program, Multiple Data
	- Master-Worker
	- Map-reduce

- <b>Globally Sequential, Locally Parallel (GSLP)</b>: this means that the application executes as a sequential program, with individual parts of it running in parallel when requested. Patterns that falls in this category include:
	- Fork/join
	- Loop parallelism

**![](https://lh4.googleusercontent.com/c9o7Lw3-a2zNH1GKxGAtjy38WFATtAuzrMA6y1mQqXOC2dwg_6wr9UMWrMWbDzpZc5HSrGXeV1lmD3RMfjuF5SyZ4jeebk9xkJ_H97wf7W_3GVpUFkyZWdH87-vfjCtVdOzNR75T18Hl3kbXzCtPfQ=s2048)**

### Single Program Multiple Data (SPMD)
Keeps all the application logic in a single program, the typical program structure involves:
- *Program initialization*: e.g. runtime initialization
- *Obtaining a unique identifier*: identifiers are numbered from 0, enumerating threads or processes used. Some systems use vector identifiers (e.g. CUDA).
- *Running the program*: execution path diversified based on ID.
- *Shutting down the program*: clean-up, saving results, etc.

### Multiple Program Multiple Data (MPMD)
SPMD fails when memory requirements are too *high* for all nodes, and when *heterogeneous* platforms are involved. MPMD Execution steps are identical to SPMD, but *deployment* involves different programs.
Both SPMD and MPMD are supported by MPI.

### Master-Worker
Two kinds of components: Master and Workers.
*Master* (one or more) is responsible for:
- Handing out pieces of work to workers.
- Collecting the results of the computations from the workers.
- Performing I/O duties on behalf of the workers, i.e. sending them the data that they are supposed to process, or accessing a file.
- Interacting with the user.

### Map-Reduce
It is a variation of the master-worker pattern used by Google's search engine implementation.
The master coordinates the whole operation, and workers run two types of tasks:
- *Map*: apply a function on data, resulting in a set of partial results
- *Reduce*: collect the partial results and derive the complete one.
Map and reduce workers can vary in number.

**![Architettura di MapReduce](https://lh5.googleusercontent.com/G9kih3EXuSCmajdeY0hmRCn1DSL4x19v1u644Fqsjtk0-wb6ZQUuoX-UKmd8I0gdpOZiN0MOQBAkoV8ezxURy3KIcmpFkuMERkb9EbDlyFbNAGOUPeimxdGJ6pmoW-10UUFTWEVrX13PFWDvo3Z70g=s2048)**
Here the master is called *JobTracker*, and workers (*TaskTracker*) collect data from distributed file systems. Workers perform both map and reduce operations and the Master coordinates the whole operation. It works usually with shared-nothing data and are those data considered as <key, value> pairs. The framework is responsible for creating, allocating, and managing masters and workers.

### Fork/Join
With this method, we can dynamically create children tasks at run-time and those tasks may run via spawning of threads, or via use of a static pool of threads. Children's tasks have to be finished for the parent thread to continue.
###### Ex.
```python
mergesort(A, lo, hi):
	#at least one element of input
	if lo<hi: 
		mid = [lo+(hi-lo)/2]
		#process potentially in parallel with main task
		fork mergesort(A, lo, mid)
		#main task handles second recursion
		mergesort(A, mid, hi)
		join
		merge(A, lo, mid, hi)
```
