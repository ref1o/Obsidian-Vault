## #Chapter Why parallel computing?

- From 1986 – 2002, microprocessors were speeding like a rocket, increasing in performance by an average of 50% per year.  
- Since then, it’s dropped to about a 20% increase per year.

Instead of designing and building faster microprocessors, we tried to put *multiple* processors on a single integrated circuit, but adding more of them doesn't help if programmers aren't aware of them or don't know how to use them.
*Serial* programs don't benefit from this approach (in most cases).

Up to now, performance increases have been attributable to the increasing density of transistors, but there are inherent problems: smaller transistors -> faster processors -> increased power consumption -> increased heat ->unreliable processors.
The solution was to move away from *single-core* systems to *multicore* processors, introducing *parallelism*.

---
###### Ex. we need to compute `n` values and add them together.
Serial solution:
```c
sum = 0;
for (i = 0; i<n; i++){
	x = ComputeNextValue(...);
	sum += x;
}
```

Now we have `p` cores, with `p` much smaller than `n`, so each core performs a partial sum of approximately `n/p` values.
```c
my_sum = 0;
my_first_i = ...;
my_last_i = ...;
for (my_i = my_first_i; my_i < my_last_i; my_i++){
	my_x = Compute_next_value(...);
	my_sum += my_x;
}
```
Each core uses its own private variables and executes this block of code independently of the other cores. Once all the cores are done computing their private *my_sum*, they form a global sum by sending results to a designated *master* core which adds the final result.

```c
if(I_m the master core){
	sum = my_x;
	for each core other than myself{
		receive value from core;
		sum += value;
	}
} else {
	send my_x to the master;
}
```
But we don't need to make the master core do all the work: we could pair the cores so that core `0` adds its result with core `1`'s result, core `2` adds its result with core `3`, etc. and this works with both odd and even-numbered pairs of cores.
Repeat the process now with only the evenly ranked cores: so core `0` adds result from core `2`, core `4` adds result from core `6`, etc.
Now cores divisible by 4 repeat the process, and so forth, until core `0` has the final result (looks like a *tree structure*).

---
### Coordination
Cores usually need to *coordinate* their work
- *Communication* - one or more cores send their current partial sums to another one.
- *Load balancing* - share the work evenly (the best you can) among the cores so that one is not heavily loaded.
- *Synchronization* - because each core works at its own pace, make sure cores do not get too far ahead of the rest
---
### Type of parallel systems
- Shared-memory 
	- The cores can share access to the computer's memory.
	- Coordinate the cores by having them examine and update shared memory locations.
- Distributed-memory
	- Each core has its own, private memory.
	- The cores must communicate explicitly by sending messages across a network.

---
## #Chapter Parallel Hardware and Parallel Software

The *main memory* is a collection of *locations*, each capable of storing instructions and data. Every location consists of an *address*, which is used to access the location, and the contents of the location.

The *Central Processing Unit* (CPU) is divided into two parts:
- *Control unit* - responsible for deciding which instruction in a program should be executed.
- *Arithmetic and logic unit* (ALU) - responsible for executing the actual instructions.

### Multitasking
Gives an illusion that a single processor system is running multiple programs simultaneously, but in reality what happens is that each process takes *turns* running (time slice), after its time's up, it waits until it has a turn again. (blocks)

### Threading
*Threads* are contained within processes, allowing programmers to divide their programs into (more or less) independent tasks. The hope is that when one thread blocks because it is waiting for a resource, another one will have work to do and can run.

### Basics of caching
A CPU cache is typically located on the same chip or can be accessed much faster than ordinary memory.

When a CPU writes data to the cache, the value in it may be inconsistent with the value in the main memory.
- *Write-through* caches handle this by updating the data in the main memory at the time it is written to the cache.
- *Write-back* caches mark data in the cache as *dirty*. When the cache line is replaced by a new cache line from memory, the *dirty* line is written to memory.

### Virtual memory
If we run a very large program or a program that accesses very large data sets, all of the instructions and data may not fit into main memory, so *virtual memory* functions as a cache for secondary storage. It exploits the principle of spatial and temporal locality and it only keeps the active parts of running programs in the main memory.
###### Swap space
Those parts that are idle are kept in a block of secondary storage.
###### Pages
Blocks of data and instructions that usually are relatively large; most systems have a fixed page size that currently ranges from 4 to 16kb.

### Virtual page numbers
When a program is compiled, its pages are assigned *virtual* page numbers. When the program is running, a *table* is created that maps the virtual page numbers to physical addresses. A page table is used to translate the virtual address into a physical address.

### Translation-lookaside buffer (TLB)
Using a page table has the potential to significantly increase each program's overall run time. The *TLB* is a special address translation cache in the processor; it caches a small number of entries (typically 16-512) from the page table in *very fast* memory.

### Pipeling
###### Ex.
```c
float x[1000], y[1000], z[1000]
...
for(i = 0; i < 1000; i++){
	z[i] = x[i] + y[i];
}
```
Assume each operation takes one nanosecond ($10^{-9}$ seconds), this `for` loop takes about 7000 nanoseconds. Now divide the floating point adder into 7 separate pieces of hardware of functional units: the first one fetches two operands, the second unit compares exponents, etc.; the output of one functional unit is input to the next one.

One floating point addition still takes 7 nanoseconds, but 1000 floating point additions now take 1006 nanoseconds.


[[index]]